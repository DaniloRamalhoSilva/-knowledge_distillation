{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) Setup\n"
      ],
      "metadata": {
        "id": "fUgSGRDCUHWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install scikit-learn pandas tqdm iterative-stratification\n"
      ],
      "metadata": {
        "id": "o5MFRpwsT7yu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, math, random, gc, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "MClFMuhXUCI2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Paths & classes"
      ],
      "metadata": {
        "id": "TAXq2znjUPUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ajuste este caminho para sua máquina/Colab\n",
        "ROOT = Path('NIH_ChestXray14')  # ex: '/content/drive/MyDrive/NIH_ChestXray14'\n",
        "CSV_PATH = ROOT / 'Data_Entry_2017.csv'\n",
        "IMAGES_DIR = ROOT / 'images'\n",
        "\n",
        "# Classes exigidas no enunciado, respeitando o \"Nodule Mass\" combinado\n",
        "TARGETS = [\n",
        "    'Atelectasis','Consolidation','Infiltration','Pneumothorax','Edema',\n",
        "    'Emphysema','Fibrosis','Effusion','Pneumonia','Pleural_thickening',\n",
        "    'Cardiomegaly','Nodule Mass','Hernia'\n",
        "]\n",
        "\n",
        "IMG_EXTS = {'.png', '.jpg', '.jpeg'}\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1JAhVofUR2k",
        "outputId": "dc902e6e-e869-4104-eda8-a250dfc0df9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cae02fa8d50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Carregar CSV e filtrar apenas pelas imagens existentes"
      ],
      "metadata": {
        "id": "iOHhRvs_UX_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# normaliza nomes de colunas principais do NIH\n",
        "df = df.rename(columns={'Image Index':'image_name', 'Finding Labels':'labels'})\n",
        "\n",
        "# imagens realmente presentes na pasta\n",
        "available = {\n",
        "    p.name for p in IMAGES_DIR.iterdir()\n",
        "    if p.is_file() and p.suffix.lower() in IMG_EXTS\n",
        "}\n",
        "\n",
        "# filtra df\n",
        "df = df[df['image_name'].isin(available)].reset_index(drop=True)\n",
        "len(df), df.head(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8defTER5UYmN",
        "outputId": "06682a28-95ba-4fb2-c96c-6253eb476a4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4193,\n",
              "          image_name              labels  Follow-up #  Patient ID  Patient Age  \\\n",
              " 0  00000028_000.png  Pleural_Thickening            0          28           63   \n",
              " 1  00000029_000.png          No Finding            0          29           59   \n",
              " 2  00000030_000.png         Atelectasis            0          30           74   \n",
              " \n",
              "   Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
              " 0              M            PA                 2048     2500   \n",
              " 1              F            PA                 2500     2048   \n",
              " 2              M            PA                 2992     2991   \n",
              " \n",
              "    OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
              " 0                        0.168  0.168          NaN  \n",
              " 1                        0.171  0.171          NaN  \n",
              " 2                        0.143  0.143          NaN  )"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Parse dos rótulos e mapa para as 13 classes (combinando “Nodule” e “Mass”)"
      ],
      "metadata": {
        "id": "3gN3j7FpUczu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_labels(s: str):\n",
        "    # 'No Finding' => vetor zero\n",
        "    if s == 'No Finding':\n",
        "        return set()\n",
        "    return set([x.strip() for x in s.split('|') if x.strip()])\n",
        "\n",
        "def row_to_multihot(orig_set: set):\n",
        "    # mapping 1:1 exceto \"Nodule Mass\" := (Nodule OR Mass)\n",
        "    vals = []\n",
        "    for c in TARGETS:\n",
        "        if c == 'Nodule Mass':\n",
        "            vals.append( int(('Nodule' in orig_set) or ('Mass' in orig_set)) )\n",
        "        elif c == 'Pleural_thickening':\n",
        "            # NIH usa 'Pleural_Thickening' (T maiúsculo às vezes). Normaliza:\n",
        "            vals.append( int(('Pleural_Thickening' in orig_set) or ('Pleural_thickening' in orig_set)) )\n",
        "        else:\n",
        "            vals.append( int(c in orig_set) )\n",
        "    return vals\n",
        "\n",
        "df['label_set'] = df['labels'].map(parse_labels)\n",
        "Y = np.vstack(df['label_set'].map(row_to_multihot).values)\n",
        "Y.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omSNxRErUdxY",
        "outputId": "2b729c34-8d39-4528-c06e-3bbe9fdf94a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4193, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Split multirrótulo estratificado (train/val/test)"
      ],
      "metadata": {
        "id": "ch2A5znnUgQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# proporções (ajuste se quiser)\n",
        "train_prop, val_prop, test_prop = 0.7, 0.15, 0.15\n",
        "n_splits = int(1/val_prop)  # aproxima para pegar uma fold ~ val_prop\n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "# 1º split: train vs val-candidate\n",
        "idx = np.arange(len(df))\n",
        "train_idx, val_idx = next(mskf.split(idx, Y))\n",
        "\n",
        "X_train, Y_train = idx[train_idx], Y[train_idx]\n",
        "X_valcand, Y_valcand = idx[val_idx], Y[val_idx]\n",
        "\n",
        "# 2º split do \"val-candidate\": val vs test\n",
        "mskf2 = MultilabelStratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n",
        "val_idx2, test_idx2 = next(mskf2.split(X_valcand, Y_valcand))\n",
        "\n",
        "val_idx_final  = X_valcand[val_idx2]\n",
        "test_idx_final = X_valcand[test_idx2]\n",
        "\n",
        "len(X_train), len(val_idx_final), len(test_idx_final)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpavLNQ1Uigi",
        "outputId": "d604d162-df97-4bca-d8f5-e1a0bc8e476b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3494, 350, 349)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Dataset & DataLoader (com augmentations leves e 3 canais)"
      ],
      "metadata": {
        "id": "I6dxvCXdUmdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),   # X-ray mono → 3 canais\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]) # ImageNet stats\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "class ChestXrayDataset(Dataset):\n",
        "    def __init__(self, df, indices, targets, img_dir, tfm):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.indices = indices\n",
        "        self.targets = targets\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.tfm = tfm\n",
        "    def __len__(self): return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        ridx = self.indices[i]\n",
        "        row = self.df.iloc[ridx]\n",
        "        img = Image.open(self.img_dir/row['image_name']).convert('L') # robustez\n",
        "        img = self.tfm(img)\n",
        "        y = torch.tensor(self.targets[ridx], dtype=torch.float32)\n",
        "        return img, y\n",
        "\n",
        "train_ds = ChestXrayDataset(df, X_train, Y, IMAGES_DIR, train_tfms)\n",
        "val_ds   = ChestXrayDataset(df, val_idx_final, Y, IMAGES_DIR, val_tfms)\n",
        "test_ds  = ChestXrayDataset(df, test_idx_final, Y, IMAGES_DIR, val_tfms)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "6G64-dCMUoB5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Modelos: Teacher = ResNet50, Student = MobileNetV2"
      ],
      "metadata": {
        "id": "eEa_JdtvUqef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = len(TARGETS)\n",
        "\n",
        "def build_resnet50(num_classes=NUM_CLASSES, pretrained=True):\n",
        "    m = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
        "    in_features = m.fc.in_features\n",
        "    m.fc = nn.Linear(in_features, num_classes)\n",
        "    return m\n",
        "\n",
        "def build_mobilenet_v2(num_classes=NUM_CLASSES, pretrained=True):\n",
        "    m = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V2 if pretrained else None)\n",
        "    in_features = m.classifier[-1].in_features\n",
        "    m.classifier[-1] = nn.Linear(in_features, num_classes)\n",
        "    return m\n",
        "\n",
        "teacher = build_resnet50().to(DEVICE)\n",
        "student = build_mobilenet_v2().to(DEVICE)\n"
      ],
      "metadata": {
        "id": "niLM7zwXUrmf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Knowledge Distillation (KL com temperatura + BCE dura)"
      ],
      "metadata": {
        "id": "bZt0MetfUtks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) instalar\n",
        "!pip -q install torchxrayvision\n",
        "\n",
        "# 1) carregar o teacher pronto (NIH, 224x224)\n",
        "import torchxrayvision as xrv\n",
        "\n",
        "# densenet NIH; pode usar também: \"densenet121-res224-all\" ou \"resnet50-res512-all\"\n",
        "teacher_raw = xrv.models.DenseNet(\n",
        "    weights=\"densenet121-res224-nih\",\n",
        "    apply_sigmoid=False  # queremos LOGITS p/ KD\n",
        ").to(DEVICE).eval()\n",
        "\n",
        "# 2) checar as saídas do modelo (ordem das patologias)\n",
        "print(teacher_raw.targets)\n",
        "# ['Atelectasis','Consolidation','Infiltration','Pneumothorax','Edema','Emphysema','Fibrosis',\n",
        "#  'Effusion','Pneumonia','Pleural_Thickening','Cardiomegaly','Nodule','Mass','Hernia',\n",
        "#  'Lung Lesion','Fracture','Lung Opacity','Enlarged Cardiomediastinum']\n",
        "\n",
        "# 3) criar um wrapper que mapeia as 18 saídas do teacher → suas 13 TARGETS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "TARGETS = [\n",
        "    'Atelectasis','Consolidation','Infiltration','Pneumothorax','Edema',\n",
        "    'Emphysema','Fibrosis','Effusion','Pneumonia','Pleural_thickening',\n",
        "    'Cardiomegaly','Nodule Mass','Hernia'\n",
        "]\n",
        "\n",
        "# mapa de índices do teacher (18) para as suas 13\n",
        "idx_map = { name:i for i,name in enumerate(teacher_raw.targets) }\n",
        "\n",
        "sel_indices = [\n",
        "    idx_map['Atelectasis'],\n",
        "    idx_map['Consolidation'],\n",
        "    idx_map['Infiltration'],\n",
        "    idx_map['Pneumothorax'],\n",
        "    idx_map['Edema'],\n",
        "    idx_map['Emphysema'],\n",
        "    idx_map['Fibrosis'],\n",
        "    idx_map['Effusion'],\n",
        "    idx_map['Pneumonia'],\n",
        "    idx_map['Pleural_Thickening'],  # seu alvo chama Pleural_thickening (normalizamos no parse)\n",
        "    idx_map['Cardiomegaly'],\n",
        "    # \"Nodule Mass\" = OR(Nodule, Mass) nos LOGITS: aprox. via log-sum-exp dos dois\n",
        "    (idx_map['Nodule'], idx_map['Mass']),\n",
        "    idx_map['Hernia']\n",
        "]\n",
        "\n",
        "class Teacher13(nn.Module):\n",
        "    def __init__(self, base):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "    def forward(self, x):\n",
        "        logits18 = self.base(x)  # [B,18] (LOGITS se apply_sigmoid=False)\n",
        "        outs = []\n",
        "        for item in sel_indices:\n",
        "            if isinstance(item, tuple):\n",
        "                # combinar Nodule/Mass em um único logit via log-sum-exp(sigmoid^-1)\n",
        "                # logits → probs p1,p2 ; OR: p_or = 1 - (1-p1)(1-p2)\n",
        "                p = torch.sigmoid(logits18[:, item[0]]), torch.sigmoid(logits18[:, item[1]])\n",
        "                p_or = 1 - (1 - p[0]) * (1 - p[1])\n",
        "                # volta a \"logit\" p/ KD: log(p/(1-p))\n",
        "                logit_or = torch.log(p_or/(1-p_or+1e-8) + 1e-8)\n",
        "                outs.append(logit_or.unsqueeze(1))\n",
        "            else:\n",
        "                outs.append(logits18[:, item].unsqueeze(1))\n",
        "        return torch.cat(outs, dim=1)  # [B,13]\n",
        "\n",
        "teacher = Teacher13(teacher_raw).to(DEVICE).eval()\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad_(False)  # congela o teacher\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FILk7-T1Uwd1",
        "outputId": "d176a4f9-3673-4300-9c61-284f1192a8db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Mass', 'Hernia', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) Avaliação final no test set"
      ],
      "metadata": {
        "id": "ElDGJuV3Vp-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student.load_state_dict(torch.load('student_best.pt', map_location=DEVICE))\n",
        "y_true, y_prob, aucs, mauc = evaluate(student, test_dl)\n",
        "\n",
        "print('mAUROC (test):', round(mauc,4))\n",
        "for c, a in zip(TARGETS, aucs):\n",
        "    print(f'{c:>20s}: {a:.4f}' if not np.isnan(a) else f'{c:>20s}: N/A (sem positivos no split)')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ESo7Z34AVqyB",
        "outputId": "06d54005-94df-4050-b4f0-7418ad2cca56"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'student_best.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1867099093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'student_best.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mAUROC (test):'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTARGETS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maucs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'student_best.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) Inferência em novas imagens"
      ],
      "metadata": {
        "id": "BzcCzd5eVviy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_image(path, model, threshold=0.5):\n",
        "    model.eval()\n",
        "    img = Image.open(path).convert('L')\n",
        "    x = val_tfms(img).unsqueeze(0).to(DEVICE)\n",
        "    logits = model(x)\n",
        "    prob = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "    pred = (prob >= threshold).astype(int)\n",
        "    return {cls: float(p) for cls, p in zip(TARGETS, prob)}, {cls: int(v) for cls, v in zip(TARGETS, pred)}\n",
        "\n",
        "# exemplo\n",
        "probs, preds = predict_image(IMAGES_DIR/'00000001_000.png', student, threshold=0.5)\n",
        "probs, preds\n"
      ],
      "metadata": {
        "id": "YgPQiTgfVuyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmakvw46pZxR"
      },
      "source": [
        "# Knowledge Distillation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kxav25upZxT"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DSWe0esUpZxU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path('NIH_ChestXray14')  # ajuste se estiver no Colab/local\n",
        "IMG_DIRS = [ROOT/'imagens']     # pode colocar várias pastas aqui\n",
        "\n",
        "valid_exts = {'.png', '.jpg', '.jpeg'}\n",
        "available_files = set()\n",
        "\n",
        "for d in IMG_DIRS:\n",
        "    for p in d.glob('*'):\n",
        "        if p.suffix.lower() in valid_exts:\n",
        "            available_files.add(p.name)\n",
        "\n",
        "len(available_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOgl2Z8BNhMZ",
        "outputId": "b4dce960-58d9-4e54-8f1c-3897f919be66"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1111"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = ROOT/'Data_Entry_2017.csv'  # ajuste o caminho\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# coluna do NIH: 'Image Index' (cada linha = uma imagem)\n",
        "df_sub = df[df['Image Index'].isin(available_files)].copy()\n",
        "print(df.shape, '->', df_sub.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyu2tXUiNou6",
        "outputId": "4b66547d-211b-4180-ef85-4b4fa0d70fc3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(112120, 12) -> (1111, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ec2_CPx7ulHj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "outputId": "454556c9-b35f-4237-fa9c-bada5b881e86"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1972064603.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1972064603.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    kaggle datasets download -d nih-chest-xrays/data -f Data_Entry_2017.csv -p .\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "#path = kagglehub.dataset_download(\"nih-chest-xrays/data\")\n",
        "\n",
        "#print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mQwMcKTpZxV"
      },
      "source": [
        "## Construindo a classe `Distiller()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vZhL1D8XpZxV"
      },
      "outputs": [],
      "source": [
        "# ==== Distiller para Multi-Label (BCE + KD com temperatura) ====\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import ops\n",
        "\n",
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(self, optimizer, metrics,\n",
        "                student_loss_fn, distillation_loss_fn,\n",
        "                alpha=0.5, temperature=3.0):\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        # Forward teacher (congelado)\n",
        "        teacher_pred = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            student_pred = self.student(x, training=True)\n",
        "\n",
        "            # Student supervised loss (BCE multi-label)\n",
        "            student_loss = self.student_loss_fn(y, student_pred)\n",
        "\n",
        "            # Distillation loss: KL entre probabilidades suavizadas\n",
        "            t = self.temperature\n",
        "            teacher_soft = ops.sigmoid(teacher_pred / t)\n",
        "            student_soft = ops.sigmoid(student_pred / t)\n",
        "            distill_loss = self.distillation_loss_fn(teacher_soft, student_soft) * (t**2)\n",
        "\n",
        "            loss = self.alpha * student_loss + (1.0 - self.alpha) * distill_loss\n",
        "\n",
        "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
        "\n",
        "        # Atualiza métricas no espaço multi-label\n",
        "        for m in self.metrics:\n",
        "            m.update_state(y, ops.sigmoid(student_pred))\n",
        "        return {\"loss\": loss, \"student_loss\": student_loss, \"distill_loss\": distill_loss, **{m.name: m.result() for m in self.metrics}}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        y_pred = self.student(x, training=False)\n",
        "        s_loss = self.student_loss_fn(y, y_pred)\n",
        "        for m in self.metrics:\n",
        "            m.update_state(y, ops.sigmoid(y_pred))\n",
        "        return {\"loss\": s_loss, **{m.name: m.result() for m in self.metrics}}\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.student(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS-rFrqMpZxV"
      },
      "source": [
        "## Criando modelos student and teacher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NvKFEstPpZxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aecc2e7-30d2-4227-eb27-fbc7ef04bd48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# ==== Modelos: teacher grande (ResNet50) e student menor (MobileNetV2) ====\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_teacher(input_shape=(224,224,3), num_classes=NUM_CLASSES):\n",
        "    base = keras.applications.ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
        "    base.trainable = True  # pode usar fine-tuning\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = keras.applications.resnet50.preprocess_input(inputs)\n",
        "    x = base(x, training=True)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    logits = layers.Dense(num_classes, name=\"logits\")(x)  # sem ativação\n",
        "    return keras.Model(inputs, logits, name=\"teacher_resnet50\")\n",
        "\n",
        "def build_student(input_shape=(224,224,3), num_classes=NUM_CLASSES):\n",
        "    base = keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
        "    base.trainable = True\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "    x = base(x, training=True)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    logits = layers.Dense(num_classes, name=\"logits\")(x)  # sem ativação\n",
        "    return keras.Model(inputs, logits, name=\"student_mobilenetv2\")\n",
        "\n",
        "teacher = build_teacher()\n",
        "student = build_student()\n",
        "\n",
        "# clone p/ comparação do student treinado \"from scratch\"\n",
        "student_scratch = keras.models.clone_model(student)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V02RBKvcpZxW"
      },
      "source": [
        "## Preparando o dataset\n",
        "\n",
        "[MNIST](https://keras.io/api/datasets/mnist/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ulBwVOF7pZxW"
      },
      "outputs": [],
      "source": [
        "# ==== NIH Chest X-ray 14: preparação de dados (multi-label) ====\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Caminhos (ajuste para a sua máquina)\n",
        "BASE_DIR = \"NIH_ChestXray14\"   # <- ajuste\n",
        "CSV_PATH = f\"{BASE_DIR}/Data_Entry_2017.csv\"\n",
        "IMG_DIR  = f\"{BASE_DIR}/images\"      # as imagens ficam aqui\n",
        "\n",
        "# As 13 doenças exigidas (notar que o dataset original tem 'Mass' e 'Nodule' separados)\n",
        "TARGETS_13 = [\n",
        "    \"Atelectasis\",\"Consolidation\",\"Infiltration\",\"Pneumothorax\",\"Edema\",\n",
        "    \"Emphysema\",\"Fibrosis\",\"Effusion\",\"Pneumonia\",\"Pleural_Thickening\",\n",
        "    \"Cardiomegaly\",\"Nodule Mass\",\"Hernia\"\n",
        "]\n",
        "\n",
        "# Observação importante:\n",
        "# O NIH tem 'Mass' e 'Nodule' como rótulos distintos. O enunciado pede \"Nodule Mass\" (junto).\n",
        "# A regra abaixo unifica: se houver 'Nodule' OU 'Mass', marcamos 'Nodule Mass'.\n",
        "# (Se você preferir manter separados, basta tratar como 14 classes e remover esta unificação.)\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, encoding='latin-1')\n",
        "df.rename(columns={\"Finding Labels\":\"Finding_Labels\", \"Image Index\":\"Image\"}, inplace=True)\n",
        "\n",
        "# Filtra apenas imagens que tenham ao menos um rótulo das 13 categorias (ignorando 'No Finding')\n",
        "def normalize_label_string(s):\n",
        "    return [x.strip() for x in s.split('|') if x.strip()]\n",
        "\n",
        "def to_multihot_13(labels):\n",
        "    labels = set(labels)\n",
        "    # unificação Nodule/Mass:\n",
        "    has_nodule_mass = ('Nodule' in labels) or ('Mass' in labels)\n",
        "\n",
        "    hot = {k:0 for k in TARGETS_13}\n",
        "    for d in labels:\n",
        "        if d in [\"Atelectasis\",\"Consolidation\",\"Infiltration\",\"Pneumothorax\",\"Edema\",\n",
        "                 \"Emphysema\",\"Fibrosis\",\"Effusion\",\"Pneumonia\",\"Pleural_Thickening\",\n",
        "                 \"Cardiomegaly\",\"Hernia\"]:\n",
        "            hot[d] = 1\n",
        "    if has_nodule_mass:\n",
        "        hot[\"Nodule Mass\"] = 1\n",
        "    return np.array([hot[k] for k in TARGETS_13], dtype=np.float32)\n",
        "\n",
        "df[\"labels_list\"] = df[\"Finding_Labels\"].apply(normalize_label_string)\n",
        "df = df[df[\"labels_list\"].map(lambda L: L and L != [\"No Finding\"])].copy()\n",
        "df[\"y\"] = df[\"labels_list\"].apply(to_multihot_13)\n",
        "\n",
        "# split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "IMG_SIZE = (224, 224)   # redes pré-treinadas tipicamente usam 224x224\n",
        "BATCH = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_image(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=1)       # imagens são em escala de cinza\n",
        "    img = tf.image.grayscale_to_rgb(img)              # repetir canal p/ 3 canais\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "def make_ds(dataframe, training=True):\n",
        "    paths = tf.constant([f\"{IMG_DIR}/{fn}\" for fn in dataframe[\"Image\"].tolist()])\n",
        "    labels = tf.stack(list(dataframe[\"y\"].values))\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    def _map(p, y):\n",
        "        x = load_image(p)\n",
        "        if training:\n",
        "            # augment leve (opcional)\n",
        "            x = tf.image.random_flip_left_right(x)\n",
        "        return x, y\n",
        "    ds = ds.shuffle(4096) if training else ds\n",
        "    ds = ds.map(_map, num_parallel_calls=AUTOTUNE).batch(BATCH).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_ds(train_df, training=True)\n",
        "val_ds   = make_ds(val_df, training=False)\n",
        "\n",
        "NUM_CLASSES = len(TARGETS_13)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "781e809f"
      },
      "source": [
        "NUM_CLASSES = len(TARGETS_13)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDAXlFqlpZxW"
      },
      "source": [
        "## Treinando o teacher\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VInja3jspZxW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "44fd4097-1bff-40c1-e105-058614cef30a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) NOT_FOUND:  NIH_ChestXray14/images/00009302_015.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) NOT_FOUND:  NIH_ChestXray14/images/00009302_015.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_49690]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2202195676.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# aumente épocas se tiver GPU/tempo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) NOT_FOUND:  NIH_ChestXray14/images/00009302_015.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) NOT_FOUND:  NIH_ChestXray14/images/00009302_015.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_49690]"
          ]
        }
      ],
      "source": [
        "# ==== Treino do TEACHER (multi-label) ====\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-4),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[AUC(curve=\"ROC\", multi_label=True, num_labels=NUM_CLASSES, name=\"auroc\")],\n",
        ")\n",
        "\n",
        "teacher.fit(train_ds, validation_data=val_ds, epochs=5)   # aumente épocas se tiver GPU/tempo\n",
        "teacher.evaluate(val_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUN-j1JapZxX"
      },
      "source": [
        "## Distill teacher para student\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBWDsn_HpZxX"
      },
      "outputs": [],
      "source": [
        "# ==== Distillation: teacher -> student ====\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-4),\n",
        "    metrics=[AUC(curve=\"ROC\", multi_label=True, num_labels=NUM_CLASSES, name=\"auroc\")],\n",
        "    student_loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.5,          # 0.3–0.7 costuma funcionar bem\n",
        "    temperature=3.0,    # 2–5\n",
        ")\n",
        "distiller.fit(train_ds, validation_data=val_ds, epochs=5)\n",
        "distiller.evaluate(val_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtAQjHOqpZxX"
      },
      "source": [
        "## Treinando o student 'from scratch' para comparação\n",
        "\n",
        "We can also train an equivalent student model from scratch without the teacher, in order\n",
        "to evaluate the performance gain obtained by knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z48bdt5PpZxX"
      },
      "outputs": [],
      "source": [
        "# ==== Student do zero (baseline p/ comparação) ====\n",
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-4),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[AUC(curve=\"ROC\", multi_label=True, num_labels=NUM_CLASSES, name=\"auroc\")],\n",
        ")\n",
        "student_scratch.fit(train_ds, validation_data=val_ds, epochs=5)\n",
        "student_scratch.evaluate(val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTT4joq1r5oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kF8r3RCw5mK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faça a Destilação de um modelo grande pré-treinado, exemplo (VGG, ResNet, ou treinado do zero) para um modelo menor aplicado na resolução do seguinte problema: https://www.kaggle.com/datasets/nih-chest-xrays/data\n",
        "\n",
        "Neste problema, o seu modelo deverá indicar para cada imagem de RaioX do tórax uma das seguintes doenças(Utilize a coluna Finding Labels do arquivo Data_Entry_2017.csv como variável target):\n",
        "\n",
        "Atelectasis\n",
        "Consolidation\n",
        "Infiltration\n",
        "Pneumothorax\n",
        "Edema\n",
        "Emphysema\n",
        "Fibrosis\n",
        "Effusion\n",
        "Pneumonia\n",
        "Pleural_thickening\n",
        "Cardiomegaly\n",
        "Nodule Mass\n",
        "Hernia\n",
        "\n"
      ],
      "metadata": {
        "id": "waucEAQXHuYS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBpWrtltJN6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}